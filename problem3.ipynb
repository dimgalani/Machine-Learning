{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQ7YzabwjTmk"
   },
   "source": [
    "## Εργασία 3 ##\n",
    "\n",
    "Καλωσήρθατε στην τρίτη σας εργασία. Η εργασία αυτή έχει σκοπό να σας βοηθήσει να εμπεδώσετε τα σύνολα μοντέλων.\n",
    "\n",
    "Στην εργασία αυτή θα πρέπει να συμπληρώσετε κώδικα Python 3 στα σημεία που αναφέρουν # YOUR CODE HERE. Μην τροποποιείτε τον κώδικα που βρίσκεται εκτός αυτών των περιοχών.\n",
    "\n",
    "Πρωτού παραδόσετε την εργασία σας σιγουρευτείτε ότι ο κώδικας σε όλα τα κελιά τρέχει σωστά. Για το σκοπό αυτό επιλέξτε από το μενού Χρόνος εκτέλεσης (runtime) -> Επανεκίνηση περιόδου λειτουργίας και εκτέλεση όλων.\n",
    "\n",
    "Συμπληρώστε το όνομα (NAME) και το AEM σας παρακάτω:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3-rBqXXbjyR0"
   },
   "outputs": [],
   "source": [
    "NAME = \"DIMITRA GALANI\"\n",
    "AEM = \"10331\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egArYhcsTG-T"
   },
   "source": [
    "**1** Διαβάστε το διαθέσιμο από την sklearn σύνολο δεδομένων breast cancer, χωρίστε το σε δεδομένα εκπαίδευσης (X_train, y_train) και ελέγχου (X_test, y_test) σε ποσοστό 70%/30% αντίστοιχα με τη συνάρτηση train_test_split (τιμή για random_state βάλτε 0). Το σύνολο αφορά τη διάγνωση καρκίνου του μαστού με βάση μεταβλητές που υπολογίζονται από μια ψηφιοποιημένη εικόνα δείγματος μάζας μαστού που λήφθηκε μέσω αναρρόφησης λεπτής βελόνας (FNA). (2 μονάδες)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "id": "qgaPtNAmTCX7",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "29b99b6039fac516d5fa9c7649e40e1d",
     "grade": false,
     "grade_id": "cell-407a2dea48bfbf80",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the breast cancer dataset\n",
    "breast_cancer = load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# Split the dataset into 70% training and 30% testing  sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "19LlgZx5cOLP",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4b7add4e469cfc1221bdad01497d404",
     "grade": true,
     "grade_id": "cell-7387a72f2393ed9a",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Τεστ ορθής ανάγνωσης και διαχωρισμού του συνόλου δεδομένων\"\"\"\n",
    "assert round(X_train[0][8], 5) == 0.1779\n",
    "assert round(X_test[0][8], 5) == 0.2116"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RB8RexuPciQr"
   },
   "source": [
    "**2** Υλοποιήστε μια ντετερμινιστική εκδοχή της μεθόδου των τυχαίων υποχώρων, η οποία χτίζει τόσα μοντέλα όσες και οι μεταβλητές εισόδου, κάθε ένα από τα οποία αγνοεί και μία διαφορετική μεταβλητή εισόδου. Π.χ. το πρώτο μοντέλο αγνοεί την πρώτη, το δεύτερο αγνοεί τη δεύτερη κτλ. Χρησιμοποιήστε τη συνάρτηση clone από το sklearn.base για να δημιουργήστε αντίγραφο του βασικού μοντέλου σε κάθε επανάληψη. (4 μονάδες)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "id": "KuC_s04KcigR",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5fbe3d9c3d8e46ffc9d9cf06a7644fa3",
     "grade": false,
     "grade_id": "cell-df57dc0d540a2518",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "class RandomSubspaceDet:\n",
    "    def __init__(self, estimator=DecisionTreeClassifier()):\n",
    "        self.base_estimator = estimator # The base estimator to train on\n",
    "        self.models = [] # The trained models\n",
    "        self.features_to_ignore = [] # The features to ignore for each model\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        n_features = X_train.shape[1] # The number of features is the number of columns in X_train\n",
    "        self.models = [] # Reset the models\n",
    "        self.features_to_ignore = [] # Reset the features to ignore\n",
    "\n",
    "        for i in range(n_features):\n",
    "            # Create a copy of the base estimator at each iteration\n",
    "            model = clone(self.base_estimator)\n",
    "            # Store which feature to ignore\n",
    "            self.features_to_ignore.append(i)\n",
    "            X_train_subset = np.delete(X_train, i, axis=1) # Exclude the ith feature\n",
    "            model.fit(X_train_subset, y_train)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        # Initialize the predictions array\n",
    "        predictions = np.zeros((n_samples, len(self.models)))\n",
    "\n",
    "        # Collect predictions from each model\n",
    "        for i, model in enumerate(self.models):\n",
    "            # Predict using the data excluding the ith feature\n",
    "            X_subset = np.delete(X, self.features_to_ignore[i], axis=1)\n",
    "            predictions[:, i] = model.predict(X_subset)\n",
    "\n",
    "        # Choose predictions by majority vote\n",
    "        majority_votes = np.apply_along_axis(lambda x: np.bincount(x.astype(int)).argmax(), axis=1, arr=predictions)\n",
    "        return majority_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "iDNUeGUEciwi",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4db933425279be3712175509b4ba2f53",
     "grade": true,
     "grade_id": "cell-786f87fa5e67b624",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Τεστ ορθής υλοποίησης RandomSubspaceDet\"\"\"\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rs = RandomSubspaceDet(estimator=DecisionTreeClassifier(random_state=1))\n",
    "rs.fit(X_train, y_train)\n",
    "assert round(accuracy_score(rs.predict(X_test), y_test), 4) == 0.9006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n19eEYRNRnG-"
   },
   "source": [
    "**3** Υλοποιήστε τη μέθοδο AdaBoost όπως παρουσιάστηκε στο μάθημα. Χρησιμοποιήστε τη συνάρτηση clone από το sklearn.base για να δημιουργήστε αντίγραφο του βασικού μοντέλου σε κάθε επανάληψη. Χρησιμοποιήστε την παράμετρο sample_weight της fit του βασικού μοντέλου για να ορίσετε τα βάρη των παραδειγμάτων εκπαίδευσης. (4 μονάδες)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "id": "7NOoKPI8TBX6",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fe3d39eaf3d6a53e2b29f9ebd1e7b6a",
     "grade": false,
     "grade_id": "cell-946d2440bc05714e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "class AdaBoost:\n",
    "    def __init__(self, n_estimators=20, estimator=DecisionTreeClassifier(max_depth=1)):\n",
    "        self.n_estimators = n_estimators  # Number of estimators\n",
    "        self.base_estimator = estimator  # Base estimator\n",
    "        self.estimators = []  # List of trained estimators\n",
    "        self.estimator_weights = []  # Weights of the estimators\n",
    "        self.classes_ = None  # The number of classes \n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        self.classes_ = len(np.unique(y_train))  # Store unique classes used in predict function\n",
    "        n_samples = X_train.shape[0]\n",
    "        sample_weights = np.ones(n_samples) / n_samples  # Initialize sample weights L2-L4 in pseudocode\n",
    "\n",
    "        for t in range(self.n_estimators):\n",
    "            estimator = clone(self.base_estimator)\n",
    "            estimator.fit(X_train, y_train, sample_weight=sample_weights) #L6 in pseudocode\n",
    "            predictions = estimator.predict(X_train)\n",
    "\n",
    "            # Calculate the weighted error rate - L7 in pseudocode\n",
    "            incorrect = (predictions != y_train)\n",
    "            # print(sample_weights, incorrect)\n",
    "            estimator_error = np.dot(sample_weights, incorrect)\n",
    "\n",
    "            # If error is greater than 0.5 or 0, break - L8-L10 in pseudocode\n",
    "            if estimator_error >= 0.5 or estimator_error == 0:\n",
    "                break\n",
    "\n",
    "            # Update sample weights based on correct and incorrect predictions - L11-L15 in pseudocode\n",
    "            for i in range(n_samples):\n",
    "                if predictions[i] == y_train[i]:\n",
    "                    sample_weights[i] *= estimator_error / (1.0 - estimator_error)\n",
    "\n",
    "            # Normalize sample weights - L16-L18 in pseudocode\n",
    "            sample_weights /= np.sum(sample_weights)\n",
    "\n",
    "            # Store the estimator and its weight\n",
    "            self.estimators.append(estimator)\n",
    "            # Compute the estimator's weight for the predict function - L22 in pseudocode\n",
    "            estimator_weight = np.log((1.0 - estimator_error) / (estimator_error))\n",
    "            self.estimator_weights.append(estimator_weight)\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        n_samples = X.shape[0] # Number of samples\n",
    "        n_classes = self.classes_ # Number of classes\n",
    "        predictions = np.zeros((n_samples, n_classes)) # Initialize predictions array\n",
    "\n",
    "        for i, estimator in enumerate(self.estimators):\n",
    "            prediction = estimator.predict(X) # Get the predictions of the estimator\n",
    "            # Update the predictions array with the weighted predictions\n",
    "            for j in range(n_samples):\n",
    "                predictions[j, prediction[j]] += self.estimator_weights[i] \n",
    "        return np.argmax(predictions, axis=1) # Return the class with the highest weight\n",
    "\n",
    "    #! Accuracy score takes 2 arrays as input and compares them element-wise    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jVjqVcv_tmYk",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32aff6a2c5ed06a7b34e982fc295d895",
     "grade": true,
     "grade_id": "cell-88a2903df26757f7",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Τεστ ορθής υλοποίησης AdaBoost\"\"\"\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "ab = AdaBoost(n_estimators=20, estimator=DecisionTreeClassifier(max_depth=1, random_state=1))\n",
    "ab.fit(X_train, y_train)\n",
    "assert round(accuracy_score(ab.predict(X_test), y_test), 4) == 0.9591\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "0QkPMoBNz3T5",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71c5a522427b4fc254b65b5d431a767d",
     "grade": false,
     "grade_id": "cell-4f6f954c3531f480",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'estimator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\Dimitra\\AppData\\Local\\Temp\\ipykernel_27976\\872399745.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Ίδιο αποτέλεσμα και με τη κλάση της sklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"SAMME\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.9591\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'estimator'"
     ]
    }
   ],
   "source": [
    "# Ίδιο αποτέλεσμα και με τη κλάση της sklearn\n",
    "ab = AdaBoostClassifier(n_estimators=20, algorithm=\"SAMME\", estimator=DecisionTreeClassifier(max_depth=1, random_state=1))\n",
    "ab.fit(X_train, y_train)\n",
    "assert round(accuracy_score(ab.predict(X_test), y_test), 4) == 0.9591"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
